{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59614613",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(\"Current Working Directory:\", cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avocado\n",
    "\n",
    "classifier = avocado.load_classifier('redshift_weight')\n",
    "features_path = \"plasticc_augment\"\n",
    "dataset = avocado.load(features_path, metadata_only=True)\n",
    "dataset.load_raw_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0116b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = dataset.metadata\n",
    "data = dataset.load_raw_features()\n",
    "data = data.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1997709",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = classifier.classifiers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf38574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "Y = metadata['class']\n",
    "Y_encoded = le.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4248a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectiveFeaturizer(avocado.plasticc.PlasticcFeaturizer):\n",
    "    def select_features(self, raw_features):\n",
    "        return pd.read_pickle(\"experiments/reduced_data.pkl\")\n",
    "\n",
    "class GenerativeFeaturizer(avocado.plasticc.PlasticcFeaturizer):\n",
    "    def select_features(self, raw_features):\n",
    "        # generating the features avocado found to be best\n",
    "        original_features = super().select_features(raw_features)\n",
    "\n",
    "        # Adding aditional features\n",
    "        cleaned = raw_features.dropna(axis=1)\n",
    "        cleaned = cleaned.drop(['ra', 'decl', 'mwebv', 'ddf', 'count'], axis=1)\n",
    "        \n",
    "        extra_features = self.select_another_set_features(cleaned)\n",
    "\n",
    "        # Creating a unified dataframe\n",
    "        combined = pd.concat([original_features, extra_features], axis=1)\n",
    "        combined = combined.loc[:, ~combined.columns.duplicated()]\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    from sklearn.base import BaseEstimator, TransformerMixin\n",
    "    \n",
    "    class PairwiseCombinations(BaseEstimator, TransformerMixin):\n",
    "        \n",
    "        def __init__(self, operations=[\"add\", \"sub\", \"mul\", \"div\"]):\n",
    "            self.operations = operations\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.features_ = X.select_dtypes(include=np.number).columns.tolist()\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            from itertools import combinations\n",
    "            \n",
    "            X = X.copy()\n",
    "            new_features = []\n",
    "\n",
    "            # Get combinations of numeric columns\n",
    "            pairs = list(combinations(self.features_, 2))\n",
    "\n",
    "            for col1, col2 in pairs:\n",
    "                idx = X.index  # Ensure all new Series use same index\n",
    "                if \"add\" in self.operations:\n",
    "                    new_features.append(pd.Series(X[col1] + X[col2], name=f\"{col1}_plus_{col2}\", index=idx))\n",
    "                if \"sub\" in self.operations:\n",
    "                    new_features.append(pd.Series(X[col1] - X[col2], name=f\"{col1}_minus_{col2}\", index=idx))\n",
    "                if \"mul\" in self.operations:\n",
    "                    new_features.append(pd.Series(X[col1] * X[col2], name=f\"{col1}_mul_{col2}\", index=idx))\n",
    "                if \"div\" in self.operations:\n",
    "                    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "                        new_features.append(pd.Series(X[col1] / X[col2].replace(0, np.nan), name=f\"{col1}_div_{col2}\", index=idx))\n",
    "\n",
    "            # Combine all new columns at once\n",
    "            if new_features:\n",
    "                new_df = pd.concat(new_features, axis=1)\n",
    "                return pd.concat([X, new_df], axis=1)\n",
    "\n",
    "            return X  # fallback: return original if nothing new\n",
    "\n",
    "    def select_another_set_features(self, raw_features):\n",
    "        transformer = self.PairwiseCombinations(operations=['div'])\n",
    "        return transformer.fit_transform(raw_features).dropna(axis=1)\n",
    "\n",
    "dataset = avocado.load(features_path, metadata_only=True)\n",
    "dataset.load_raw_features()\n",
    "    \n",
    "classifier = avocado.LightGBMClassifier(\n",
    "        'test',\n",
    "        SelectiveFeaturizer(),\n",
    "        class_weights=None,\n",
    "        weighting_function=avocado.evaluate_weights_redshift,\n",
    ")\n",
    "classifier.train(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4062fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = dataset.select_features(SelectiveFeaturizer())\n",
    "model_1 = classifier.classifiers[0]\n",
    "explainer_1 = shap.TreeExplainer(model_1)\n",
    "shap_1 = explainer_1.shap_values(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9969ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "corr = X1.corr(method='pearson')\n",
    "corr = 1 - corr.abs()\n",
    "\n",
    "clustering = AgglomerativeClustering(\n",
    "    affinity='precomputed',\n",
    "    linkage='complete',\n",
    "    distance_threshold=0.3,\n",
    "    n_clusters=None\n",
    ")\n",
    "labels = clustering.fit_predict(corr)\n",
    "np.unique(labels).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b69e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# This dict will store selected features per class\n",
    "class_to_top_features = defaultdict(set)\n",
    "\n",
    "cluster_id_to_features = {}\n",
    "for col, cluster_id in zip(X1.columns, labels):\n",
    "    cluster_id_to_features.setdefault(cluster_id, []).append(col)\n",
    "\n",
    "# Loop over each class\n",
    "for class_id in np.unique(Y):\n",
    "    mask = (Y == class_id)\n",
    "    \n",
    "    # Get SHAP values for this class\n",
    "    shap_values_class = shap_1[class_map[class_id]][mask]\n",
    "    this_df = pd.DataFrame(shap_values_class, columns=X1.columns)\n",
    "\n",
    "    # Compute median SHAP per feature\n",
    "    median_shap = this_df.median().abs()\n",
    "\n",
    "    # Aggregate by cluster\n",
    "    cluster_medians = {}\n",
    "    for cluster_id, features in cluster_id_to_features.items():\n",
    "        valid_features = [f for f in features if f in median_shap.index]\n",
    "        if valid_features:\n",
    "            cluster_medians[cluster_id] = median_shap[valid_features].mean()\n",
    "\n",
    "    # Sort clusters by importance\n",
    "    cluster_shap_series = pd.Series(cluster_medians).sort_values(ascending=False)\n",
    "    normalized_shap = cluster_shap_series / cluster_shap_series.sum()\n",
    "    cumulative_shap = normalized_shap.cumsum()\n",
    "\n",
    "    # Get top clusters contributing to 99% SHAP\n",
    "    top_clusters = cumulative_shap[cumulative_shap <= 0.99].index\n",
    "    if len(cumulative_shap) > len(top_clusters):\n",
    "        top_clusters = cumulative_shap[cumulative_shap <= 0.99].append(\n",
    "            cumulative_shap[cumulative_shap > 0.99].iloc[:1]\n",
    "        ).index\n",
    "\n",
    "    # Add features from top clusters\n",
    "    for cluster in top_clusters:\n",
    "        class_to_top_features[class_id].update(cluster_id_to_features[cluster])\n",
    "\n",
    "# Union of all top features across classes\n",
    "all_top_features = set().union(*class_to_top_features.values())\n",
    "\n",
    "final_top_features = set()\n",
    "\n",
    "for class_id in np.unique(Y):\n",
    "    mask = (Y == class_id)\n",
    "    \n",
    "    # Get SHAP values and median SHAP\n",
    "    shap_values_class = shap_1[class_map[class_id]][mask]\n",
    "    this_df = pd.DataFrame(shap_values_class, columns=X1.columns)\n",
    "    median_shap = this_df.median().abs()\n",
    "    \n",
    "    for cluster_id, features in cluster_id_to_features.items():\n",
    "        # Check if any feature from this cluster is in the selected set for this class\n",
    "        if any(f in class_to_top_features[class_id] for f in features):\n",
    "            valid_features = [f for f in features if f in median_shap.index]\n",
    "            if valid_features:\n",
    "                # Add top features to the united set\n",
    "                top_k = median_shap[valid_features].sort_values(ascending=False).head(1).index\n",
    "                final_top_features.update(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607c8fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = dataset.select_features(avocado.plasticc.PlasticcFeaturizer())\n",
    "\n",
    "common_columns = list(set(X2.columns) & set(X1[final_top_features].columns))\n",
    "not_in_common = list(set(X2.columns) - set(X1[final_top_features].columns))\n",
    "\n",
    "X2[not_in_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b99b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X1\n",
    "test.to_pickle(\"experiments/reduced_data.pkl\")\n",
    "test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "astro_fin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
